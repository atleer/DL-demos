{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03c4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b87113",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "\n",
    "- Perform convolution of kernel across image to reduce dimensionality of image and share parameters (of neighboring parts of the image).\n",
    "- The weights of the kernel(s) are optimized during training. As are the weights in the fully connected layers that follow the convolutional layers.\n",
    "- Formula for calculating dimensionality of output after a convolution (with no padding and stride = 1):\n",
    "\n",
    "$$\n",
    "H_{out} = H_{image} - H_{kernel} + 1\n",
    "W_{out} = W_{image} - W_{kernel} + 1\n",
    "$$\n",
    "\n",
    "where H is the height and W is the width of the 2D matrix.\n",
    "\n",
    "- The formula when stride, padding, and dilation are added to the kernels can be found [here](https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html). Here's a [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) to a nice illustration of dilation.\n",
    "\n",
    "- In general, the convolutional layers are followed by a fully connected layer (dense network of neurons). But in some cases, for example segmentation, there's not fully connected layers and only convolutional layers. In that case, up convolution (\"reverse convolution\") is added at the end to get the image as the output again, because the purpose is to get the identity - the object it belongs to - of every pixel in the image. This is called a Fully Convolutional Network (FCN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abebdb",
   "metadata": {},
   "source": [
    "#### Building a CNN:\n",
    "\n",
    "- First, without the fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe543441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional neural network class.\n",
    "    When an instance of it is constructed with a kernel, you can apply that instance\n",
    "        to a matrix and it will convolve the kernel over that image.\n",
    "    i.e. Net(kernel)(image)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel=None, padding=0):\n",
    "        \"\"\"\n",
    "            Summary of the nn.conv2d parameters (you can also get this by hovering\n",
    "        over the method):\n",
    "        - in_channels (int): Number of channels in the input image\n",
    "        - out_channels (int): Number of channels produced by the convolution\n",
    "        - kernel_size (int or tuple): Size of the convolving kernel\n",
    "\n",
    "        Args:\n",
    "        padding: int or tuple, optional\n",
    "            Zero-padding added to both sides of the input. Default: 0\n",
    "        kernel: np.ndarray\n",
    "            Convolving kernel. Default: None\n",
    "\n",
    "        Returns:\n",
    "        Nothing\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 2, padding = padding)\n",
    "\n",
    "        # Set up a default kernel if none is provided\n",
    "        if kernel is not None:\n",
    "            dim1, dim2 = kernel.shape\n",
    "            kernel = kernel.reshape(1,1,dim1,dim2)\n",
    "            self.conv1.weight = torch.nn.Parameter(kernel)\n",
    "            self.conv1.bias = torch.nn.Parameter(torch.zeros_like(self.conv1.bias))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward Pass of nn.conv2d\n",
    "\n",
    "        Args:\n",
    "        x: torch.tensor\n",
    "            Input features\n",
    "\n",
    "        Returns:\n",
    "        x: torch.tensor\n",
    "            Convolution output\n",
    "        \"\"\"\n",
    "        return self.conv1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55423ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:\n",
      "tensor([[[[0., 1., 2.],\n",
      "          [3., 4., 5.],\n",
      "          [6., 7., 8.]]]])\n",
      "Kernel:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "Output:\n",
      "tensor([[[[19., 25.],\n",
      "          [37., 43.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "kernel = torch.Tensor(np.arange(4).reshape(2,2))\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "net = Net(kernel=kernel, padding=0).to(device=DEVICE)\n",
    "\n",
    "# 3x3 image matrix with numbers from 0 through 8\n",
    "image = torch.Tensor(np.arange(9).reshape(3,3))\n",
    "image = image.reshape(1,1,3,3).to(device=DEVICE)\n",
    "\n",
    "print(\"Image:\\n\" + str(image))\n",
    "print(\"Kernel:\\n\" + str(kernel))\n",
    "output = net(image)  # Apply the convolution\n",
    "print(\"Output:\\n\" + str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "215c3d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image (before padding):\n",
      "tensor([[[[0., 1., 2.],\n",
      "          [3., 4., 5.],\n",
      "          [6., 7., 8.]]]])\n",
      "Kernel:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "Output:\n",
      "tensor([[[[ 0.,  3.,  8.,  4.],\n",
      "          [ 9., 19., 25., 10.],\n",
      "          [21., 37., 43., 16.],\n",
      "          [ 6.,  7.,  8.,  0.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Image (before padding):\\n\" + str(image))\n",
    "print(\"Kernel:\\n\" + str(kernel))\n",
    "\n",
    "# Prepare the network with the aforementioned default kernel, but this\n",
    "# time with padding\n",
    "net = Net(kernel=kernel, padding=1).to(DEVICE)\n",
    "output = net(image)  # Apply the convolution onto the padded image\n",
    "print(\"Output:\\n\" + str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d2a89",
   "metadata": {},
   "source": [
    "- A CNN with the fully connected network and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xvs0_dataset(normalize=False, download=False):\n",
    "  \"\"\"\n",
    "  Load Dataset\n",
    "\n",
    "  Args:\n",
    "    normalize: boolean\n",
    "      If true, normalise dataloader\n",
    "    download: boolean\n",
    "      If true, download dataset\n",
    "\n",
    "  Returns:\n",
    "    emnist_train: torch.loader\n",
    "      Training Data\n",
    "    emnist_test: torch.loader\n",
    "      Test Data\n",
    "  \"\"\"\n",
    "  if normalize:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "  else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "  emnist_train = datasets.EMNIST(root='./data',\n",
    "                                 split='letters',\n",
    "                                 download=download,\n",
    "                                 train=True,\n",
    "                                 transform=transform)\n",
    "  emnist_test = datasets.EMNIST(root='./data',\n",
    "                                split='letters',\n",
    "                                download=download,\n",
    "                                train=False,\n",
    "                                transform=transform)\n",
    "\n",
    "  # Only want O (15) and X (24) labels\n",
    "  train_idx = (emnist_train.targets == 15) | (emnist_train.targets == 24)\n",
    "  emnist_train.targets = emnist_train.targets[train_idx]\n",
    "  emnist_train.data = emnist_train.data[train_idx]\n",
    "\n",
    "  # Convert Xs predictions to 1, Os predictions to 0\n",
    "  emnist_train.targets = (emnist_train.targets == 24).type(torch.int64)\n",
    "\n",
    "  test_idx = (emnist_test.targets == 15) | (emnist_test.targets == 24)\n",
    "  emnist_test.targets = emnist_test.targets[test_idx]\n",
    "  emnist_test.data = emnist_test.data[test_idx]\n",
    "\n",
    "  # Convert Xs predictions to 1, Os predictions to 0\n",
    "  emnist_test.targets = (emnist_test.targets == 24).type(torch.int64)\n",
    "\n",
    "  return emnist_train, emnist_test\n",
    "\n",
    "\n",
    "def get_data_loaders(train_dataset, test_dataset,\n",
    "                     batch_size=32, seed=0):\n",
    "  \"\"\"\n",
    "  Helper function to fetch dataloaders\n",
    "\n",
    "  Args:\n",
    "    train_dataset: torch.tensor\n",
    "      Training data\n",
    "    test_dataset: torch.tensor\n",
    "      Test data\n",
    "    batch_size: int\n",
    "      Batch Size\n",
    "    seed: int\n",
    "      Set seed for reproducibility\n",
    "\n",
    "  Returns:\n",
    "    emnist_train: torch.loader\n",
    "      Training Data\n",
    "    emnist_test: torch.loader\n",
    "      Test Data\n",
    "  \"\"\"\n",
    "  g_seed = torch.Generator()\n",
    "  g_seed.manual_seed(seed)\n",
    "\n",
    "  train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=2,\n",
    "                            worker_init_fn=seed_worker,\n",
    "                            generator=g_seed)\n",
    "  test_loader = DataLoader(test_dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True,\n",
    "                           num_workers=2,\n",
    "                           worker_init_fn=seed_worker,\n",
    "                           generator=g_seed)\n",
    "\n",
    "  return train_loader, test_loader\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness.\n",
    "  NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39752bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 562M/562M [05:32<00:00, 1.69MB/s] \n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(SEED)\n",
    "emnist_train, emnist_test = get_Xvs0_dataset(normalize=True, download=True)\n",
    "train_loader, test_loader = get_data_loaders(emnist_train, emnist_test,\n",
    "                                             seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9adf0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epochs):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "      model: nn.module\n",
    "        Neural network instance\n",
    "      device: string\n",
    "        GPU/CUDA if available, CPU otherwise\n",
    "      epochs: int\n",
    "        Number of epochs\n",
    "      train_loader: torch.loader\n",
    "        Training set\n",
    "\n",
    "    Returns:\n",
    "      losses\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1E-2)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "      with tqdm(train_loader, unit='batch') as tepoch:\n",
    "         for data, target in tepoch:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "            losses.append(loss.item())\n",
    "            time.sleep(0.1)\n",
    "\n",
    "def test(model, device, test_loader, criterion = nn.CrossEntropyLoss()):\n",
    "   \n",
    "  model.eval()\n",
    "   \n",
    "  losses = []\n",
    "  for data, labels in test_loader:\n",
    "    data = data.to(device).float()\n",
    "    labels = labels.to(device).long()\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "      \n",
    "  acc = 100*correct / total\n",
    "\n",
    "  return acc\n",
    "   \n",
    "    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dda842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in Network    1198850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:15<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m emnist_net = EMNIST_Net().to(DEVICE)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal Parameters in Network \u001b[39m\u001b[38;5;132;01m{:10d}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28msum\u001b[39m(p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m emnist_net.parameters())))\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43memnist_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m## Uncomment to test your model\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTest accuracy is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest(emnist_net,\u001b[38;5;250m \u001b[39mDEVICE,\u001b[38;5;250m \u001b[39mtest_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, epochs)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     24\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, unit=\u001b[33m'\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tepoch:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43m     \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtepoch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\multiprocessing\\queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\multiprocessing\\connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\multiprocessing\\connection.py:346\u001b[39m, in \u001b[36mPipeConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    344\u001b[39m             _winapi.PeekNamedPipe(\u001b[38;5;28mself\u001b[39m._handle)[\u001b[32m0\u001b[39m] != \u001b[32m0\u001b[39m):\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\multiprocessing\\connection.py:1084\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1081\u001b[39m                 ready_objects.add(o)\n\u001b[32m   1082\u001b[39m                 timeout = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m     ready_handles = \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1086\u001b[39m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\multiprocessing\\connection.py:1016\u001b[39m, in \u001b[36m_exhaustive_wait\u001b[39m\u001b[34m(handles, timeout)\u001b[39m\n\u001b[32m   1014\u001b[39m ready = []\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m     res = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res == WAIT_TIMEOUT:\n\u001b[32m   1018\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class EMNIST_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network instance with following structure\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3) # Convolutional Layer 1\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3) + max-pooling # Convolutional Block 2\n",
    "    nn.Linear(in_features=9216, out_features=128) # Fully Connected Layer 1\n",
    "    nn.Linear(in_features=128, out_features=2) # Fully Connected Layer 2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EMNIST_Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=9216, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # the 1 means start flattening from dimension 1 and keep dim 0 as is\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "emnist_net = EMNIST_Net().to(DEVICE)\n",
    "print(\"Total Parameters in Network {:10d}\".format(sum(p.numel() for p in emnist_net.parameters())))\n",
    "train(emnist_net, DEVICE, train_loader, 1)\n",
    "## Uncomment to test your model\n",
    "print(f'Test accuracy is: {test(emnist_net, DEVICE, test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26c7027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x33856 and 9216x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m plt.imshow(emnist_train[x_img_idx][\u001b[32m0\u001b[39m].reshape(\u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m),\n\u001b[32m      8\u001b[39m            cmap=plt.get_cmap(\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      9\u001b[39m plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m output = \u001b[43memnist_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m result = F.softmax(output, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResult:\u001b[39m\u001b[33m\"\u001b[39m, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mEMNIST_Net.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     26\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pool(x)\n\u001b[32m     27\u001b[39m x = torch.flatten(x, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# the 1 means start flattening from dimension 1 and keep dim 0 as is\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m x = F.relu(x)\n\u001b[32m     30\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1x33856 and 9216x128)"
     ]
    }
   ],
   "source": [
    "# Index of an image in the dataset that corresponds to an X and O\n",
    "x_img_idx = 11\n",
    "o_img_idx = 0\n",
    "\n",
    "print(\"Input:\")\n",
    "x_img = emnist_train[x_img_idx][0].unsqueeze(dim=0).to(DEVICE)\n",
    "plt.imshow(emnist_train[x_img_idx][0].reshape(28, 28),\n",
    "           cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "output = emnist_net(x_img)\n",
    "result = F.softmax(output, dim=1)\n",
    "print(\"\\nResult:\", result)\n",
    "print(\"Confidence of image being an 'O':\", result[0, 0].item())\n",
    "print(\"Confidence of image being an 'X':\", result[0, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19d1ff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x33856 and 9216x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m plt.imshow(emnist_train[o_img_idx][\u001b[32m0\u001b[39m].reshape(\u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m),\n\u001b[32m      4\u001b[39m            cmap=plt.get_cmap(\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      5\u001b[39m plt.show()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m output = \u001b[43memnist_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m result = F.softmax(output, dim=\u001b[32m1\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResult:\u001b[39m\u001b[33m\"\u001b[39m, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mEMNIST_Net.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     26\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pool(x)\n\u001b[32m     27\u001b[39m x = torch.flatten(x, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# the 1 means start flattening from dimension 1 and keep dim 0 as is\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m x = F.relu(x)\n\u001b[32m     30\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1x33856 and 9216x128)"
     ]
    }
   ],
   "source": [
    "print(\"Input:\")\n",
    "o_img = emnist_train[o_img_idx][0].unsqueeze(dim=0).to(DEVICE)\n",
    "plt.imshow(emnist_train[o_img_idx][0].reshape(28, 28),\n",
    "           cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "output = emnist_net(o_img)\n",
    "result = F.softmax(output, dim=1)\n",
    "print(\"\\nResult:\", result)\n",
    "print(\"Confidence of image being an 'O':\", result[0, 0].item())\n",
    "print(\"Confidence of image being an 'X':\", result[0, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2b160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
