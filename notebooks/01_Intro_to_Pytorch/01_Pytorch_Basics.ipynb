{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737f8055",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda14a0b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ca38b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7bd9c",
   "metadata": {},
   "source": [
    "## Tensors in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cf611",
   "metadata": {},
   "source": [
    "Tensors are a generalized formulation to higher dimensions of mathematical objects that we are/may be familiar with from linear algebra: scalars, vectors, and matrices. A scalar is a 0D-tensor, a vector is a 1D-tensor, and a matrix is a 2D-tensor. Tensors can have any N number of dimensions.\n",
    "\n",
    "Tensors are more than just a container for data, though, they also include information about the linear transformations between tensors.\n",
    "\n",
    "Pytorch is a library that is used instead of NumPy when working with data in Deep Learning, but it's often used in the same way as NumPy. It's also optimized to utilize the power of GPUs.\n",
    "\n",
    "Tensors can be constructed from standard python data collections like lists, tuples, and arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d8e1afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor from a list\n",
    "a = torch.tensor([0,1,2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3196cde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.1000],\n",
       "        [1.2000, 1.3000]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor from tuples\n",
    "b = ((1.0,1.1), (1.2, 1.3))\n",
    "b = torch.tensor(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ed67bf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor of from numpy array\n",
    "c = np.ones([2,3])\n",
    "c = torch.tensor(c)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e0676",
   "metadata": {},
   "source": [
    "Pytorch also provides functions to initialize arrays, like NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "79b0468f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "49ca102d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros(4)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "13ab7e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0223, 0.1689],\n",
       "        [0.2939, 0.5185],\n",
       "        [0.6977, 0.8000],\n",
       "        [0.1610, 0.2823],\n",
       "        [0.6816, 0.9152]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.rand(size = (5, 2))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d1ac8b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4159,  0.8396, -0.8265, -0.7949],\n",
       "        [-0.9528,  0.3717,  0.4087,  1.4214],\n",
       "        [ 0.1494, -0.6709, -0.2142, -0.4320]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.randn(size = (3,4))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843e3f2",
   "metadata": {},
   "source": [
    "Set seed in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "23a6708e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5410])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# now we get the same number every time (as long as torch.manual_seed(0) has been run again)\n",
    "torch.randn(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e0d65",
   "metadata": {},
   "source": [
    "#### arange and linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "83d44ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,10,step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1929bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n",
       "        4.5000, 5.0000])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0,5,steps=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3726c1",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "Mathematical operations can be performed on tensors both using built-in python operators and methods in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4cab67f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.]),\n",
       " tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = torch.zeros(5)+0.1\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0d9ada01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a+b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "099ce17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that an out parameter can be used to create a variable that contains the result\n",
    "torch.add(a, b, out=c)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "247b8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atle_\\AppData\\Local\\Temp\\ipykernel_26972\\3231070935.py:1: UserWarning: An output with one or more elements was resized since it had shape [3, 4], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\bld\\libtorch_1753839225953\\work\\aten\\src\\ATen\\native\\Resize.cpp:37.)\n",
      "  torch.multiply(a,b,out=d)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiply(a,b,out=d)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6629da6",
   "metadata": {},
   "source": [
    "### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "62c322c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.5000, dtype=torch.float64),\n",
       " tensor([2.5000, 3.5000, 4.5000], dtype=torch.float64),\n",
       " tensor([2., 5.], dtype=torch.float64))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3], [4,5,6]], dtype=float)\n",
    "a.mean(), a.mean(axis=0), a.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "06cc2928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21., dtype=torch.float64),\n",
       " tensor([5., 7., 9.], dtype=torch.float64),\n",
       " tensor([ 6., 15.], dtype=torch.float64))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(), a.sum(axis=0), a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a5f8f",
   "metadata": {},
   "source": [
    "### Matrix Operations\n",
    "\n",
    "The @ symbol or `torch.matmul()` can be used to multiply tensors. ``torch.dot()`` can only be used for 1D tensors (vectors). For transposing tenosrs, use ``Tensor.T`` or ``torch.t()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c3437c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4]]),\n",
       " tensor([[1, 1],\n",
       "         [0, 0]]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1,2],[3,4]])\n",
    "\n",
    "B = torch.tensor([[1,1], [0,0]])\n",
    "\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7a7f3c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1],\n",
       "         [3, 3]]),\n",
       " tensor([[1, 1],\n",
       "         [3, 3]]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1 = A @ B\n",
    "\n",
    "C2 = torch.matmul(input = A, other= B)\n",
    "\n",
    "C1, C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc32c923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise multiplication of matrices\n",
    "D = torch.mul(A, B)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ed548",
   "metadata": {},
   "source": [
    "#### Conversion to other Python objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577a88e",
   "metadata": {},
   "source": [
    "Converting to numpy will \"break the graph\". In other words, autograd, which is a core feature of pytorch, won't be supported any more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f0193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3ce9db14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.440000057220459, 2.440000057220459, 2)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2.44])\n",
    "x.item(), float(x), int(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635ea26",
   "metadata": {},
   "source": [
    "Tensors come with functions to do backward propagation and provides the gradients in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1f73ad4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3], dtype=float, requires_grad=True)\n",
    "\n",
    "# simulates forward propagation\n",
    "x_sum = x.sum()\n",
    "x_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e525e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dc4cea21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f6c766",
   "metadata": {},
   "source": [
    "TODO: Create a more advanced computational graph, do forward and backward propagation, and get the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036e2c7",
   "metadata": {},
   "source": [
    "#### Device\n",
    "\n",
    "As mentioned, Pytorch is optimized for use on GPUs. When constructing a tensor, there's a parameter `device` that can be used to set the device to be used to CPUs or GPUs. By default, device is set to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7c3d8721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the device of the tensor x\n",
    "x = torch.tensor([1,2,3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f1bf974a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3], device = 'cpu')\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40db11f",
   "metadata": {},
   "source": [
    "To use GPUs, set device to \"cuda\". This will only work if you have GPUs on a laptop, if you use a cloud service that offers GPU hours like google colab or kaggle, or if you run your code on a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7daa7f10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# this will produce an error\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m x.device\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atle_\\Documents\\repositories\\NeuroMatch\\DL-demos\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\cuda\\__init__.py:363\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    366\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# this will produce an error\n",
    "x = torch.tensor([1,2,3], device = 'cuda')\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1642184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
